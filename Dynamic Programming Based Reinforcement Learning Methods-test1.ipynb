{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62734EB821F446648235C480D8F5419D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Dynamic Programming Based Reinforcement Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F718A0CBCB3D4A7F877E2FD89110F82F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "In reinforcement learning, the agent aims to maixmize its cumulative reward:\n",
    "\n",
    "$$\n",
    "\\max \\sum_{t=1}^T \\mathbb{E}_{a_t \\sim \\pi(s_t), s_{t+1} \\sim p(s_{t+1} \\mid s_t, a_t), s_t \\sim p(s)}[ \\gamma^{t-1} r(s_t,a_t)].\n",
    "$$\n",
    "\n",
    "From the perspective of Bellman equation, the caculation of cumulative reward can be also formulated as:\n",
    "\n",
    "$$\n",
    "V(s_t) = \\mathbb{E}_{a \\sim \\pi(s_t)}[r(s_t,a_t) + \\gamma V(s_{t+1})].\n",
    "$$\n",
    "\n",
    "## Policy Iteration Learning\n",
    "\n",
    "Once a policy, $\\pi$, has been improved using $v_{\\pi}$ to yield a better policy, $\\pi'$, we can then compute $v_{\\pi'}$ and improve it again to yield and even better $\\pi''$. We can thus obtain a sequence of monotonically improving policies and value functions:\n",
    "\n",
    "$$\n",
    "\\pi_0 \\stackrel{\\text{E}}{\\longrightarrow} v_{\\pi_0} \\stackrel{\\text{I}}{\\longrightarrow} \\pi_1 \\stackrel{\\text{E}}{\\longrightarrow} \\dots \\stackrel{\\text{I}}{\\longrightarrow}\\pi_{\\star}\\stackrel{\\text{E}}{\\longrightarrow}v_{\\star},\n",
    "$$\n",
    "\n",
    "where $\\stackrel{\\text{E}}{\\longrightarrow}$ denotes a policy *evaluation*, and $\\stackrel{\\text{I}}{\\longrightarrow}$ denotes a policy improvement. This way of finding an optimal policy is called *policy iteration*.\n",
    "\n",
    "### Pesudo of Policy Iteration Learning\n",
    "\n",
    "1. **Initialization**\n",
    "\n",
    "    $V(s) \\in \\mathbb{R}$ and $\\pi(s) \\in \\mathcal{A}(s)$ arbitrarily for all $s \\in \\mathcal{S}$\n",
    "\n",
    "2. **Policy Evaluation**\n",
    "\n",
    "    Loop:\n",
    "\n",
    "    > $\\Delta \\leftarrow 0$  \n",
    "    > Loop for each $s \\in \\mathcal{S}$:  \n",
    "    >> $v \\leftarrow V(S)$  \n",
    "    >> $V(s) \\leftarrow \\sum_{s',r}p(s', r | s, \\pi(s))[r + \\gamma V(s')]$  \n",
    "    >> $\\Delta \\leftarrow \\max(\\Delta, |v-V(s)|)$\n",
    "    \n",
    "    until $\\Delta < \\theta$ (a small positive number determining the accuracy of estimation)\n",
    "\n",
    "3. **Policy Improvement**  \n",
    "\n",
    "    policy-stable $\\leftarrow$ true  \n",
    "    For each $s \\in \\mathcal{S}$:\n",
    "    \n",
    "    > *old-action* $\\leftarrow \\pi(s)$  \n",
    "    > $\\pi(s) \\leftarrow \\arg \\max_a\\sum_{s', r}p(s', r | s, a)[r + \\gamma V(s')]$  \n",
    "    > If *old-action* $\\ne \\pi(s)$, then *policy-stable* $\\leftarrow$ *false*  \n",
    "    \n",
    "    If *policy-stable*, then stop and return $V \\sim v_{\\star}$ and $\\pi \\sim \\pi_{\\star}$; else go to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1759BEB80575403BBDAA3FFC4A6E4613",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**✨ Policy Evaluation Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "83DD2549C96249848B7836D0BE1E0EA4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xiaogouzi\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "def policy_eval(env, values, policies, upper_bound):\n",
    "    print('\\n===== Policy Evalution =====')\n",
    "    delta = upper_bound\n",
    "    iteration = 0\n",
    "\n",
    "    while delta >= upper_bound:\n",
    "        delta = 0.\n",
    "\n",
    "        for s in env.states:\n",
    "            v = values.get(s)\n",
    "            env.set_state(s)\n",
    "\n",
    "            action_index = policies.sample(s)\n",
    "            action = env.actions[action_index]\n",
    "            _, _, rewards, next_states = env.step(action)\n",
    "\n",
    "            next_values = values.get(list(next_states))\n",
    "            td_values = list(map(lambda x, y: x + env.gamma * y, rewards, next_values))\n",
    "\n",
    "            exp_value = np.mean(td_values)\n",
    "            values.update(s, exp_value)\n",
    "\n",
    "            # update delta\n",
    "            delta = max(delta, abs(v - exp_value))\n",
    "            \n",
    "        iteration += 1\n",
    "        print('\\r> iteration: {} delta: {}'.format(iteration, delta), flush=True, end=\"\")\n",
    "        \n",
    "\n",
    "print(\"xiaogouzi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B325523E8DC41BE898C7B6E3BD1DFDC",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**✨ Policy Improvement Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "AF407655A82D410EB0A5F0A2E22CB50A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def policy_improve(env, values, policies):\n",
    "    print('\\n===== Policy Improve =====')\n",
    "    policy_stable = True\n",
    "    \n",
    "    for state in env.states:\n",
    "        old_act = policies.sample(state)\n",
    "\n",
    "        # calculate new policy execution\n",
    "        actions = env.actions\n",
    "        value = [0] * len(env.actions)\n",
    "        \n",
    "        for i, action in enumerate(actions):\n",
    "            env.set_state(state)\n",
    "            _, _, rewards, next_states = env.step(action)\n",
    "            next_values = values.get(list(next_states))\n",
    "            td_values = list(map(lambda x, y: x + env.gamma * y, rewards, next_values))\n",
    "            prob = [1 / len(next_states)] * len(next_states)\n",
    "\n",
    "            value[i] = sum(map(lambda x, y: x * y, prob, td_values))\n",
    "\n",
    "        # action selection\n",
    "        new_act = actions[np.argmax(value)]\n",
    "\n",
    "        # greedy update policy\n",
    "        new_policy = [0.] * env.action_space\n",
    "        new_policy[new_act] = 1.\n",
    "        policies.update(state, new_policy)\n",
    "\n",
    "        if old_act != new_act:\n",
    "            policy_stable = False\n",
    "\n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24E3256556D44B4BA03DF52861D27E59",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Value Iteration\n",
    "\n",
    "One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. Must we wait for exact convergence, or can we stop short of that?\n",
    "\n",
    "### Pesudo of Value Iteration Learning\n",
    "\n",
    "Algorithm parameter: a small threshold $\\theta > 0$ determining accuracy of estimation\n",
    "\n",
    "Initialize $V(s)$, for all $s \\in \\mathcal{S}^+$, arbitrarily except that $V(\\textit{terminal})=0$\n",
    "\n",
    "Loop:\n",
    "\n",
    ">  $\\Delta \\leftarrow 0$  \n",
    ">  Loop for each $s \\in \\mathcal{S}$:  \n",
    ">> $v \\leftarrow V(s)$  \n",
    ">> $V(s) \\leftarrow \\max_{a}\\sum_{s',r}p(s', r | s, a)\\Big[ r + \\gamma V(s') \\Big]$  \n",
    ">> $\\Delta \\leftarrow \\max(\\Delta, |v-V(s)|)$  \n",
    "\n",
    "> until $\\Delta < \\theta$  \n",
    "\n",
    "Output a deterministic policy, $\\pi \\approx \\pi_{\\star}$, such that $\\pi(s) = \\arg \\max_{a}\\sum_{s',r}p(s', r | s, a)\\Big[ r + \\gamma V(s') \\Big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EDE28BFDF3E4AE18FF552954522CC98",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**✨ Value Iteration Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5F6AF416A2CB443AA9A0D204A9789B3F",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def value_iter(env, values, upper_bound):\n",
    "    print('===== Value Iteration =====')\n",
    "    delta = upper_bound + 1.\n",
    "    states = copy(env.states)\n",
    "    \n",
    "    iteration = 0\n",
    "\n",
    "    while delta >= upper_bound:\n",
    "        delta = 0\n",
    "\n",
    "        for s in states:\n",
    "            v = values.get(s)\n",
    "\n",
    "            # get new value\n",
    "            actions = env.actions\n",
    "            vs = [0] * len(actions)\n",
    "\n",
    "            for i, action in enumerate(actions):\n",
    "                env.set_state(s)\n",
    "                _, _, rewards, next_states = env.step(action)\n",
    "                td_values = list(map(lambda x, y: x + env.gamma * y, rewards, values.get(next_states)))\n",
    "\n",
    "                vs[i] = np.mean(td_values)\n",
    "\n",
    "            values.update(s, max(vs))\n",
    "            delta = max(delta, abs(v - values.get(s)))\n",
    "        \n",
    "        iteration += 1\n",
    "        print('\\r> iteration: {} delta: {}'.format(iteration, delta), end=\"\", flush=True)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0247E06F49E4E5F81B3AB43C2F9261A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A Simple Environment\n",
    "\n",
    "### MatrixEnv\n",
    "\n",
    "A simple maze game, the agent needs to learn walk from the start point to the destination (goal)\n",
    "\n",
    "![](https://storage.googleapis.com/kagglesdsdata/datasets/374051/727215/matrix_game.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1589524158&Signature=cpaCiVm8OS6yJx2KDrOWULEJ%2BBVCq9LqlSDu4nOFetTkvlY0jitrx6YPsAeDme60O0JcHELm2ASuOh1r%2BZeu%2BYIHUb8gAFhtR5AeGfFiImgHT4WuxEMJu6ddQ1Iggs0mbyXKAFzYldsAFPrrqfOvcsHZB%2BjXS0uodDhkYrQradvKYIShhI2w0SsCgEN%2BoFSuCFypdXHSejnj2h51vCL8Mcu3ELE%2BWQyrmpClh1WT%2BJTclZKVMMka0elXIju7f9IEvVJRv0dLdMzLfN4EcPMcFVqSl9okMQXCMVqDGUe32uClpRCArQux4XHtFv6idx1VUQKrvBVLp0g%2BUGlJS6ueaA%3D%3D)\n",
    "\n",
    "```python\n",
    "env = MatrixEnv()\n",
    "\n",
    "# ...\n",
    "\n",
    "# before starting a new episode, be sure to call the `env.reset()`\n",
    "env.reset()\n",
    "\n",
    "# act_index: get from policy sample\n",
    "# retrieve action from action space with `act_index`\n",
    "act = env.actions[act_index]\n",
    "\n",
    "# method step accepts action, then update the inner state, return: {None, done, rewards: list, next_states: list} \n",
    "_, done, rewards, next_states = env.step(act)\n",
    "\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "003A6A6B30FE4BD292038858F625958A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self._states = set()\n",
    "        self._state = None\n",
    "        self._actions = []\n",
    "        self._gamma = None\n",
    "        \n",
    "    @property\n",
    "    def states(self):\n",
    "        return self._states\n",
    "    \n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return self._state_shape\n",
    "    \n",
    "    @property\n",
    "    def actions(self):\n",
    "        return self._actions\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return len(self._actions)\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return self._gamma\n",
    "    \n",
    "    def _world_init(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        \"\"\"Return distribution and next states\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def set_state(self, state):\n",
    "        self._state = state\n",
    "\n",
    "\n",
    "class MatrixEnv(Env):\n",
    "    def __init__(self, height=4, width=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._action_space = 4\n",
    "        self._actions = list(range(4))\n",
    "        \n",
    "        self._state_shape = (2,)\n",
    "        self._state_shape = (height, width)\n",
    "        self._states = [(i, j) for i in range(height) for j in range(width)]\n",
    "        \n",
    "        self._gamma = 0.9\n",
    "        self._height = height\n",
    "        self._width = width\n",
    "\n",
    "        self._world_init()\n",
    "        \n",
    "    @property\n",
    "    def state(self):\n",
    "        return self._state\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return self._gamma\n",
    "    \n",
    "    def set_gamma(self, value):\n",
    "        self._gamma = value\n",
    "        \n",
    "    def reset(self):\n",
    "        self._state = self._start_point\n",
    "        \n",
    "    def _world_init(self):\n",
    "        # start_point\n",
    "        self._start_point = (0, 0)\n",
    "        self._end_point = (self._height - 1, self._width - 1)\n",
    "        \n",
    "    def _state_switch(self, act):\n",
    "        # 0: h - 1, 1: w + 1, 2: h + 1, 3: w - 1\n",
    "        if act == 0:  # up\n",
    "            self._state = (max(0, self._state[0] - 1), self._state[1])\n",
    "        elif act == 1:  # right\n",
    "            self._state = (self._state[0], min(self._width - 1, self._state[1] + 1))\n",
    "        elif act == 2:  # down\n",
    "            self._state = (min(self._height - 1, self._state[0] + 1), self._state[1])\n",
    "        elif act == 3:  # left\n",
    "            self._state = (self._state[0], max(0, self._state[1] - 1))\n",
    "\n",
    "    def step(self, act):\n",
    "        assert 0 <= act <= 3\n",
    "        \n",
    "        done = False\n",
    "        reward = 0.\n",
    "\n",
    "        self._state_switch(act)\n",
    "        \n",
    "        if self._state == self._end_point:\n",
    "            reward = 1.\n",
    "            done = True\n",
    "\n",
    "        return None, done, [reward], [self._state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F02D47319F5D49A6A33D067E5AB82A7E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Basic Data Structure for Learning\n",
    "\n",
    "### ValueTable\n",
    "\n",
    "Class `ValueTable` maintains the state value function which map state space $\\mathcal{S} \\in \\mathbb{R}^2$ to real number space $\\mathbb{R}$.\n",
    "\n",
    "**Methods**:\n",
    "\n",
    "- `update(state, value)`: update state value with given value\n",
    "- `get(state)`: return state value with given state or states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2E7E206432E54E7FACAFB93E0C72875D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ValueTable:\n",
    "    def __init__(self, env):\n",
    "        self._values = np.zeros(env.state_space)\n",
    "        \n",
    "    def update(self, s, value):\n",
    "        self._values[s] = value\n",
    "        \n",
    "    def get(self, state):\n",
    "        if type(state) == list:\n",
    "            # loop get\n",
    "            res = [self._values[s] for s in state]\n",
    "            return res\n",
    "        elif type(state) == tuple:\n",
    "            # return directly\n",
    "            return self._values[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2306F4AB4D6C4A478EE969C2AE2589E9",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Policies\n",
    "\n",
    "Class `Policies` maintains the polcies of each states\n",
    "\n",
    "**Methods**:\n",
    "\n",
    "- `sample(state)`: sample action, return action index\n",
    "- `retrieve(state)`: retrieve policy of given state\n",
    "- `update(state, policy)`: update policy of state with given policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "01066BC2E7A446709FB14DD7208B58BF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "Pi = namedtuple('Pi', 'act, prob')\n",
    "\n",
    "\n",
    "class Policies:\n",
    "    def __init__(self, env: Env):\n",
    "        self._actions = env.actions\n",
    "        self._default_policy = [1 / env.action_space] * env.action_space\n",
    "        self._policies = dict.fromkeys(env.states, Pi(self._actions, self._default_policy))\n",
    "    \n",
    "    def sample(self, state):\n",
    "        if self._policies.get(state, None) is None:\n",
    "            self._policies[state] = Pi(self._actions, self._default_policy)\n",
    "\n",
    "        policy = self._policies[state]\n",
    "        return np.random.choice(policy.act, p=policy.prob)\n",
    "    \n",
    "    def retrieve(self, state):\n",
    "        return self._policies[state].prob\n",
    "    \n",
    "    def update(self, state, policy):\n",
    "        self._policies[state] = self._policies[state]._replace(prob=policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C916FF4514C5431E9546A5AF1D277165",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Solving Matrix Game via Policy Iteration Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BF8B8578E7D2430FA9FAC9D86D690E40",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 251 delta: 2.5096933587908954e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 89 delta: 9.404610869090391e-054\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 3 delta: 6.855961323637416e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 3 delta: 4.99799580495619e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 4.048376602039383e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 3.2791850476776574e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 2.656139888657094e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 2.1514733098193517e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 1.742693380979432e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 1.4115816385995572e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 1.143381127288734e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 9.261387130976573e-06\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 7.501723576996966e-06\n",
      "===== Policy Improve =====\n",
      "\n",
      "[time consumpution]: 3.9473655223846436 s\n",
      "Evaluation: [reward] 1.0 [step] 14\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = MatrixEnv(width=8, height=8)  # TODO(ming): try different word size\n",
    "policies = Policies(env)\n",
    "values = ValueTable(env)\n",
    "upper_bound = 1e-4\n",
    "\n",
    "stable = False\n",
    "\n",
    "start = time.time()\n",
    "while not stable:\n",
    "    policy_eval(env, values, policies, upper_bound)\n",
    "    stable = policy_improve(env, values, policies)\n",
    "end = time.time()\n",
    "\n",
    "print('\\n[time consumpution]: {} s'.format(end - start))\n",
    "\n",
    "done = False\n",
    "rewards = 0\n",
    "env.reset()\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    act_index = policies.sample(env.state)\n",
    "    _, done, reward, next_state = env.step(env.actions[act_index])\n",
    "    rewards += sum(reward)\n",
    "    step += 1\n",
    "\n",
    "print('Evaluation: [reward] {} [step] {}'.format(rewards, step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DE78E0BB6D84B98806EE8ED2EF65BEC",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Solving Matrix Game via Value Iteration Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EC88F28D41A6453098598344CD4437BB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Value Iteration =====\n",
      "> iteration: 89 delta: 9.404610870067387e-059\n",
      "===== Policy Improve =====\n",
      "\n",
      "[time consumption] 1.0287024974822998s\n",
      "Evaluation: [reward] 1.0 [step] 14\n"
     ]
    }
   ],
   "source": [
    "env = MatrixEnv(width=8, height=8)  # try different word size\n",
    "policies = Policies(env)\n",
    "values = ValueTable(env)\n",
    "upper_bound = 1e-4\n",
    "\n",
    "start = time.time()\n",
    "value_iter(env, values, upper_bound)\n",
    "_ = policy_improve(env, values, policies)\n",
    "end = time.time()\n",
    "\n",
    "print('\\n[time consumption] {}s'.format(end - start))\n",
    "# print(\"===== Render =====\")\n",
    "env.reset()\n",
    "done = False\n",
    "rewards = 0\n",
    "step = 0\n",
    "while not done:\n",
    "    act_index = policies.sample(env.state)\n",
    "    _, done, reward, next_state = env.step(env.actions[act_index])\n",
    "    rewards += sum(reward)\n",
    "    step += 1\n",
    "\n",
    "print('Evaluation: [reward] {} [step] {}'.format(rewards, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ECB3E27BB80497D87EB9AF0C19559EB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
